{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sys\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from string import digits\n",
    "import string\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#READING ALL THE THREE FILES\n",
    "file_imdb=open(\"imdb_labelled.txt\",\"r\")\n",
    "imdb=[]\n",
    "for line in file_imdb:\n",
    "    line = line.split('\\n')\n",
    "    line = line[0].split('\\t')\n",
    "    line[1] = int(line[1])\n",
    "    imdb.append(line)\n",
    "file_imdb.close()\n",
    "imdb_df = pd.DataFrame(imdb, columns=[\"Summary\", \"Score\"])\n",
    "file_amazon=open(\"amazon_cells_labelled.txt\",\"r\")\n",
    "amazon=[]\n",
    "for line in file_amazon:\n",
    "    line = line.split('\\n')\n",
    "    line = line[0].split('\\t')\n",
    "    line[1] = int(line[1])\n",
    "    amazon.append(line)\n",
    "file_amazon.close()\n",
    "amazon_df = pd.DataFrame(amazon, columns=[\"Summary\", \"Score\"])\n",
    "file_yelp=open(\"yelp_labelled.txt\",\"r\")\n",
    "yelp=[]\n",
    "for line in file_yelp:\n",
    "    line = line.split('\\n')\n",
    "    line = line[0].split('\\t')\n",
    "    line[1] = int(line[1])\n",
    "    yelp.append(line)\n",
    "file_yelp.close()\n",
    "yelp_df = pd.DataFrame(yelp, columns=[\"Summary\", \"Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Count of final table: ', 3000)\n"
     ]
    }
   ],
   "source": [
    "#MERGING ALL FILES INTO ONE DATAFRAME\n",
    "final_data_df = pd.concat([imdb_df,amazon_df,yelp_df], ignore_index= True)\n",
    "print (\"Count of final table: \", final_data_df[\"Summary\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Summary  Score\n",
      "0  A veri veri veri slowmov aimless movi about a ...      0\n",
      "1  not sure who wa more lost the flat charact or ...      0\n",
      "2  attempt arti with black white and clever camer...      0\n",
      "3              veri littl music or anyth to speak of      0\n",
      "4  the best scene in the movi wa when gerardo is ...      1\n",
      "5  the rest of the movi lack art charm mean If it...      0\n",
      "6                                      wast two hour      0\n",
      "7  saw the movi today and thought it wa a good ef...      1\n",
      "8                                      A bit predict      0\n",
      "9  love the cast of jimmi buffet as the scienc te...      1\n"
     ]
    }
   ],
   "source": [
    "#DATA CLEANSING APPROACHES\n",
    "data=[]\n",
    "ps = PorterStemmer()\n",
    "lmtzr = WordNetLemmatizer()\n",
    "#data_df['Summary']=data_df['Summary'].str.lower()\n",
    "for index in range(len(final_data_df['Summary'])):\n",
    "    #NORMAL\n",
    "    #data.append([final_data_df['Summary'][index],final_data_df['Score'][index]])\n",
    "    #LOWERCASE\n",
    "    #data.append([final_data_df['Summary'][index].lower(),final_data_df['Score'][index]])\n",
    "    #No Numbers\n",
    "    #data.append([final_data_df['Summary'][index].translate(None, digits),final_data_df['Score'][index]])\n",
    "    #No Punctuation\n",
    "    #data.append([final_data_df['Summary'][index].translate(None, string.punctuation),final_data_df['Score'][index]])\n",
    "    #No Numbers AND Punctuation\n",
    "    #data.append([final_data_df['Summary'][index].translate(None, string.punctuation).translate(None, digits),final_data_df['Score'][index]])\n",
    "    #Word Tokenize\n",
    "    #words = word_tokenize(final_data_df['Summary'][index])\n",
    "    #data.append([' '.join(word for word in words),final_data_df['Score'][index]])\n",
    "    #StopWords\n",
    "    #words = [word for word in final_data_df['Summary'][index].split() if word not in stopwords.words('english')]\n",
    "    #data.append([' '.join(word for word in words),final_data_df['Score'][index]])\n",
    "    #Stemming\n",
    "    #words = word_tokenize(final_data_df['Summary'][index].translate(None, string.punctuation))\n",
    "    #words = [ps.stem(word.decode(\"utf8\")) for word in words]\n",
    "    #data.append([' '.join(word for word in words),final_data_df['Score'][index]])\n",
    "    #Lemmatizer\n",
    "\n",
    "data_df = pd.DataFrame(data, columns=[\"Summary\", \"Score\"])\n",
    "\n",
    "print (data_df[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Data train count = ', 2100)\n",
      "('Data test count = ', 900)\n",
      "\n",
      "Training Data :\n",
      "611                I believ that pitch black wa done well\n",
      "530     there are so mani problem i dont know where to...\n",
      "2787    I dont have veri mani word to say about thi pl...\n",
      "49      the film succe despit or perhap becaus of an o...\n",
      "1883                                      warn DO not buy\n",
      "Name: Summary, dtype: object\n",
      "\n",
      "Testing Data :\n",
      "1801               for the price thi wa a great deal\n",
      "1190                    the replac die in a few week\n",
      "1817      get a signal when other verizon phone wont\n",
      "251     the cinematographyif it can be call thatsuck\n",
      "2505                 I would not recommend thi place\n",
      "Name: Summary, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#SPLITTING FILES RANDOMLY - 70% TRAINING & 30% TESTING\n",
    "[Data_train,Data_test,Train_labels,Test_labels] = train_test_split(data_df['Summary'],\n",
    "                                                                   data_df['Score'] , \n",
    "                                                                   test_size=0.3, \n",
    "                                                                   random_state=42)\n",
    "print (\"Data train count = \", Data_train.count())\n",
    "print (\"Data test count = \", Data_test.count())\n",
    "\n",
    "print (\"\\nTraining Data :\")\n",
    "print (Data_train[:5])\n",
    "\n",
    "print (\"\\nTesting Data :\")\n",
    "print (Data_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(random_state=20160121, criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.73      0.72       443\n",
      "          1       0.73      0.72      0.73       457\n",
      "\n",
      "avg / total       0.72      0.72      0.72       900\n",
      "\n",
      "\n",
      "Confusion Matrix for BIGRAM:\n",
      "\t\tPrediction\n",
      "\t\tNEG\tPOS\n",
      "Actual \tNEG\t325 \t118\n",
      "\tPOS\t130 \t327\n",
      "\n",
      "Accuracy for BIGRAM\n",
      "72.4444444444\n"
     ]
    }
   ],
   "source": [
    "#Bigram Model\n",
    "bigram_vec = TfidfVectorizer(ngram_range=(1, 2),                     \n",
    "                             strip_accents='unicode',\n",
    "                             min_df=2,\n",
    "                             norm='l2')\n",
    "\n",
    "bigram_model = bigram_vec.fit(data_df['Summary'])\n",
    "bigram_train = bigram_model.transform(Data_train)\n",
    "bigram_test  = bigram_model.transform(Data_test)\n",
    "\n",
    "bigram_clf = classifier.fit(bigram_train, Train_labels)\n",
    "bigram_prediction = bigram_clf.predict(bigram_test)\n",
    "\n",
    "print (metrics.classification_report(Test_labels.values, bigram_prediction))\n",
    "\n",
    "bi_conf_mat = metrics.confusion_matrix(Test_labels.values, bigram_prediction)\n",
    "\n",
    "print (\"\\nConfusion Matrix for BIGRAM:\")\n",
    "print ('\\t\\tPrediction')\n",
    "print (\"\\t\\tNEG\\tPOS\")\n",
    "\n",
    "print \"Actual\",\"\\tNEG\\t\", bi_conf_mat[0][0], \"\\t\", bi_conf_mat[0][1]\n",
    "\n",
    "print \"\\tPOS\\t\", bi_conf_mat[1][0], \"\\t\", bi_conf_mat[1][1]\n",
    "\n",
    "bi_accuracy = (float(bi_conf_mat[0][0]+bi_conf_mat[1][1])*100/900)\n",
    "print (\"\\nAccuracy for BIGRAM\")\n",
    "print (bi_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.75      0.71       443\n",
      "          1       0.73      0.65      0.69       457\n",
      "\n",
      "avg / total       0.70      0.70      0.70       900\n",
      "\n",
      "\n",
      "Confusion Matrix for UNIGRAM:\n",
      "\t\tPrediction\n",
      "\t\tNEG\tPOS\n",
      "Actual\tNEG\t 331 \t 112\n",
      "\tPOS\t 158 \t 299\n",
      "\n",
      "Accuracy for UNIGRAM\n",
      "70.0\n"
     ]
    }
   ],
   "source": [
    "#Unigram Model\n",
    "unigram_vec = TfidfVectorizer(ngram_range=(1, 1),                     \n",
    "                             strip_accents='unicode',\n",
    "                             min_df=2,\n",
    "                             norm='l2')\n",
    "\n",
    "unigram_model = unigram_vec.fit(data_df['Summary'])\n",
    "unigram_train = unigram_model.transform(Data_train)\n",
    "unigram_test  = unigram_model.transform(Data_test)\n",
    "\n",
    "unigram_clf = classifier.fit(unigram_train, Train_labels)\n",
    "unigram_prediction = unigram_clf.predict(unigram_test)\n",
    "print (metrics.classification_report(Test_labels.values, unigram_prediction))\n",
    "\n",
    "uni_conf_mat = metrics.confusion_matrix(Test_labels.values, unigram_prediction)\n",
    "print (\"\\nConfusion Matrix for UNIGRAM:\")\n",
    "print ('\\t\\tPrediction')\n",
    "print (\"\\t\\tNEG\\tPOS\")\n",
    "print (\"Actual\\tNEG\\t\", uni_conf_mat[0][0], \"\\t\", uni_conf_mat[0][1])\n",
    "print (\"\\tPOS\\t\", uni_conf_mat[1][0], \"\\t\", uni_conf_mat[1][1])\n",
    "\n",
    "uni_accuracy = (float(uni_conf_mat[0][0]+uni_conf_mat[1][1])*100/900)\n",
    "print (\"\\nAccuracy for UNIGRAM\")\n",
    "print (uni_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.75      0.73       443\n",
      "          1       0.74      0.68      0.71       457\n",
      "\n",
      "avg / total       0.72      0.72      0.72       900\n",
      "\n",
      "\n",
      "Confusion Matrix for TRIGRAM:\n",
      "\t\tPrediction\n",
      "\t\tNEG\tPOS\n",
      "Actual\tNEG\t 334 \t 109\n",
      "\tPOS\t 144 \t 313\n",
      "\n",
      "Accuracy for TRIGRAM\n",
      "71.88888888888889\n"
     ]
    }
   ],
   "source": [
    "#Trinigram Model\n",
    "trigram_vec = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                             strip_accents='unicode',\n",
    "                             min_df=2,\n",
    "                             norm='l2')\n",
    "\n",
    "trigram_model = trigram_vec.fit(data_df['Summary'])\n",
    "trigram_train = trigram_model.transform(Data_train)\n",
    "trigram_test  = trigram_model.transform(Data_test)\n",
    "\n",
    "trigram_clf = classifier.fit(trigram_train, Train_labels)\n",
    "trigram_prediction = trigram_clf.predict(trigram_test)\n",
    "print (metrics.classification_report(Test_labels.values, trigram_prediction))\n",
    "\n",
    "tri_conf_mat = metrics.confusion_matrix(Test_labels.values, trigram_prediction)\n",
    "print (\"\\nConfusion Matrix for TRIGRAM:\")\n",
    "print ('\\t\\tPrediction')\n",
    "print (\"\\t\\tNEG\\tPOS\")\n",
    "print (\"Actual\\tNEG\\t\", tri_conf_mat[0][0], \"\\t\", tri_conf_mat[0][1])\n",
    "print (\"\\tPOS\\t\", tri_conf_mat[1][0], \"\\t\", tri_conf_mat[1][1])\n",
    "\n",
    "accuracy = (float(tri_conf_mat[0][0]+tri_conf_mat[1][1])*100/900)\n",
    "print (\"\\nAccuracy for TRIGRAM\")\n",
    "print (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive: But other than that the movie seemed to drag and the heroes didn't really work for their freedom.  \n",
      "False Negative: Overall, a delight!  \n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "fp_count=0\n",
    "fn_count=0\n",
    "false_pos=[]\n",
    "false_neg=[]\n",
    "for idx, value in Test_labels.iteritems():\n",
    "    if(value==0 and bigram_prediction[i]==1):\n",
    "        false_pos.append(\"False Positive: \"+Data_test[idx])\n",
    "        fp_count+=1\n",
    "    if(value==1 and bigram_prediction[i]==0):\n",
    "        false_neg.append(\"False Negative: \"+Data_test[idx])\n",
    "        fn_count+=1\n",
    "    i+=1\n",
    "#print (fp_count)\n",
    "#print (fn_count)\n",
    "print (false_pos[1])\n",
    "print (false_neg[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
